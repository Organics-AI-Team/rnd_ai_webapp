/**
 * AI Chat API Route with RAG Integration
 * Uses Vercel AI SDK v5 for streaming responses
 */

import { GoogleGenerativeAI } from '@google/generative-ai';
import { getEmbeddingService } from '@/lib/services/embedding';
import { NextRequest } from 'next/server';

// Allow streaming responses up to 60 seconds
export const maxDuration = 60;

// Simple system prompt that works
const AGENT_SYSTEM_PROMPTS = {
  chemical_compound: `‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏™‡∏≤‡∏£‡πÄ‡∏Ñ‡∏°‡∏µ‡πÅ‡∏•‡∏∞‡πÄ‡∏Ñ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏á‡∏≤‡∏° ‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏™‡∏≤‡∏£‡πÄ‡∏Ñ‡∏°‡∏µ‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥ ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÉ‡∏ô‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡πÄ‡∏Ñ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏á‡∏≤‡∏° ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á`
};

export async function POST(req: NextRequest) {
  const startTime = Date.now();
  try {
    const { messages, agent_type = 'chemical_compound' } = await req.json();

    if (!messages || !Array.isArray(messages)) {
      return new Response('Invalid messages format', { status: 400 });
    }

    console.log('=== AI CHAT REQUEST START ===');
    console.log('Timestamp:', new Date().toISOString());
    console.log('Received chat request:', {
      messagesCount: messages.length,
      agent_type,
      lastMessageLength: messages[messages.length - 1]?.content?.length || 0
    });

    // Check for required API key
    const geminiApiKey = process.env.GEMINI_API_KEY;
    if (!geminiApiKey) {
      console.error('Missing GEMINI_API_KEY');
      return new Response('Gemini API key not configured', { status: 500 });
    }

    // Simple system prompt (no RAG for now)
    const systemPrompt = AGENT_SYSTEM_PROMPTS[agent_type as keyof typeof AGENT_SYSTEM_PROMPTS] || AGENT_SYSTEM_PROMPTS.chemical_compound;

    const geminiStartTime = Date.now();
    console.log('ü§ñ Initializing Gemini model...');

    // Initialize Gemini - use simple direct generation
    const genAI = new GoogleGenerativeAI(geminiApiKey);
    const model = genAI.getGenerativeModel({
      model: 'gemini-2.5-flash'
    });
    const geminiInitTime = Date.now() - geminiStartTime;
    console.log(`‚úÖ Gemini model initialized in ${geminiInitTime}ms`);

    // Get the current message
    const currentMessage = messages[messages.length - 1];

    // Simple direct prompt approach
    const fullPrompt = `${systemPrompt}\n\n‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: ${currentMessage.content}\n\n‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì`;

    console.log('üí¨ Using direct generation approach');
    console.log('üìù Full prompt length:', fullPrompt.length, 'characters');
    console.log('üìÑ Full prompt preview:', fullPrompt.substring(0, 200) + '...');

    // Use direct generateContent instead of chat
    const responseStartTime = Date.now();
    console.log('üí¨ Starting direct generation...');
    console.log('üîÆ Sending prompt to Gemini...');

    let finalText = '';

    try {
      const result = await model.generateContent(fullPrompt);
      const responseInitTime = Date.now() - responseStartTime;
      console.log(`‚úÖ Direct generation completed in ${responseInitTime}ms`);

      const response = result.response;
      const text = response.text();

      console.log(`üìù Response received: ${text.length} characters`);
      console.log('üìÑ Response preview:', text.substring(0, 100) + '...');

      // Store the response for streaming simulation
      finalText = text;

      // If still empty, try a simpler prompt
      if (!text || text.length === 0) {
        console.log('üîÑ Response empty, trying simpler prompt...');
        const simpleResult = await model.generateContent('‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏≠‡∏ö‡∏ß‡πà‡∏≤ hello world');
        const simpleResponse = simpleResult.response;
        const simpleText = simpleResponse.text();
        console.log(`üìù Simple test response: ${simpleText.length} characters - "${simpleText}"`);
        finalText = simpleText || '‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡∏£‡∏∞‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πà‡∏á';
      }

    } catch (error) {
      console.error('‚ùå Error during Gemini call:', error);
      finalText = '‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö AI ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πà‡∏á';
    }

    // Create a simulated streaming response from the complete text
    const encoder = new TextEncoder();
    const stream = new ReadableStream({
      async start(controller) {
        try {
          // Get the response text that was generated above
          const responseText = finalText || '‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡∏£‡∏∞‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ';

          // Simulate streaming by sending the complete response in chunks
          const chunkSize = 20; // Send 20 characters at a time
          for (let i = 0; i < responseText.length; i += chunkSize) {
            const chunk = responseText.slice(i, i + chunkSize);
            controller.enqueue(encoder.encode(`0:"${chunk.replace(/"/g, '\\"')}"\n`));
            // Small delay to simulate streaming
            await new Promise(resolve => setTimeout(resolve, 50));
          }
          controller.close();
        } catch (error) {
          console.error('‚ùå Simulated streaming error:', error);
          controller.error(error);
        }
      }
    });

    const totalTime = Date.now() - startTime;
    console.log(`üéØ Total request processing time: ${totalTime}ms`);
    console.log('=== AI CHAT REQUEST END ===');

    return new Response(stream, {
      headers: {
        'Content-Type': 'text/plain; charset=utf-8',
        'Cache-Control': 'no-cache',
        'Connection': 'keep-alive',
      },
    });
  } catch (error) {
    const totalTime = Date.now() - startTime;
    console.error(`‚ùå Chat API error after ${totalTime}ms:`, error);
    console.error('Error stack:', error instanceof Error ? error.stack : 'No stack available');
    console.log('=== AI CHAT REQUEST FAILED ===');
    return new Response(`Internal server error: ${error instanceof Error ? error.message : 'Unknown error'}`, { status: 500 });
  }
}