/**
 * AI Chat API Route with RAG Integration
 * Uses Vercel AI SDK v5 for streaming responses
 */

import { GoogleGenerativeAI } from '@google/generative-ai';
import { getEmbeddingService } from '@/lib/services/embedding';
import { NextRequest } from 'next/server';

// Allow streaming responses up to 60 seconds
export const maxDuration = 60;

// Enhanced system prompt for direct, clear, concise, and insightful responses
const AGENT_SYSTEM_PROMPTS = {
  chemical_compound: `‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏™‡∏≤‡∏£‡πÄ‡∏Ñ‡∏°‡∏µ‡πÅ‡∏•‡∏∞‡πÄ‡∏Ñ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏á‡∏≤‡∏° ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ:

üéØ **‡∏ï‡∏≠‡∏ö‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á**: ‡πÄ‡∏Ç‡πâ‡∏≤‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ó‡∏±‡∏ô‡∏ó‡∏µ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏π‡∏î‡∏ô‡∏≥‡∏´‡∏£‡∏∑‡∏≠‡∏ó‡∏±‡∏Å‡∏ó‡∏≤‡∏¢‡∏ã‡πâ‡∏≥
üí° **‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô**: ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢ ‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏®‡∏±‡∏û‡∏ó‡πå‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô
‚ö° **‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö**: ‡∏ï‡∏≠‡∏ö‡∏™‡∏±‡πâ‡∏ô‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏π‡∏î‡πÄ‡∏¢‡∏≠‡∏∞‡πÄ‡∏Å‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
üîç **‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å**: ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡πÄ‡∏ó‡πá‡∏à‡∏à‡∏£‡∏¥‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏£‡∏£‡∏∞‡∏ß‡∏±‡∏á ‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á

‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö:
- ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
- ‡πÉ‡∏ä‡πâ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ (‚Ä¢) ‡πÅ‡∏•‡∏∞‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢ (‚úÖ ‚ö†Ô∏è üí°) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
- ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Å‡πà‡∏≠‡∏ô
- ‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏∏‡∏î‡∏™‡∏±‡πâ‡∏ô‡πÜ ‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢`
};

export async function POST(req: NextRequest) {
  const startTime = Date.now();
  try {
    const { messages, agent_type = 'chemical_compound' } = await req.json();

    if (!messages || !Array.isArray(messages)) {
      return new Response('Invalid messages format', { status: 400 });
    }

    console.log('=== AI CHAT REQUEST START ===');
    console.log('Timestamp:', new Date().toISOString());
    console.log('Received chat request:', {
      messagesCount: messages.length,
      agent_type,
      lastMessageLength: messages[messages.length - 1]?.content?.length || 0
    });

    // Check for required API key
    const geminiApiKey = process.env.GEMINI_API_KEY;
    if (!geminiApiKey) {
      console.error('Missing GEMINI_API_KEY');
      return new Response('Gemini API key not configured', { status: 500 });
    }

    // Simple system prompt (no RAG for now)
    const systemPrompt = AGENT_SYSTEM_PROMPTS[agent_type as keyof typeof AGENT_SYSTEM_PROMPTS] || AGENT_SYSTEM_PROMPTS.chemical_compound;

    const geminiStartTime = Date.now();
    console.log('ü§ñ Initializing Gemini model...');

    // Initialize Gemini - use simple direct generation
    const genAI = new GoogleGenerativeAI(geminiApiKey);
    const model = genAI.getGenerativeModel({
      model: 'gemini-2.5-flash'
    });
    const geminiInitTime = Date.now() - geminiStartTime;
    console.log(`‚úÖ Gemini model initialized in ${geminiInitTime}ms`);

    // Get the current message
    const currentMessage = messages[messages.length - 1];

    // Enhanced prompt approach for better responses
    const fullPrompt = `${systemPrompt}

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ: ${currentMessage.content}

‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏≠‡∏ö‡∏ï‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏Ç‡πâ‡∏≤‡∏á‡∏ï‡πâ‡∏ô ‡πÇ‡∏î‡∏¢‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î`;

    console.log('üí¨ Using direct generation approach');
    console.log('üìù Full prompt length:', fullPrompt.length, 'characters');
    console.log('üìÑ Full prompt preview:', fullPrompt.substring(0, 200) + '...');

    // Use direct generateContent instead of chat
    const responseStartTime = Date.now();
    console.log('üí¨ Starting direct generation...');
    console.log('üîÆ Sending prompt to Gemini...');

    let finalText = '';

    try {
      const result = await model.generateContent(fullPrompt);
      const responseInitTime = Date.now() - responseStartTime;
      console.log(`‚úÖ Direct generation completed in ${responseInitTime}ms`);

      const response = result.response;
      const text = response.text();

      console.log(`üìù Response received: ${text.length} characters`);
      console.log('üìÑ Response preview:', text.substring(0, 100) + '...');

      // Store the response for streaming simulation
      finalText = text;

      // If still empty, try a simpler prompt
      if (!text || text.length === 0) {
        console.log('üîÑ Response empty, trying simpler prompt...');
        const simpleResult = await model.generateContent('‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏≠‡∏ö‡∏ß‡πà‡∏≤ hello world');
        const simpleResponse = simpleResult.response;
        const simpleText = simpleResponse.text();
        console.log(`üìù Simple test response: ${simpleText.length} characters - "${simpleText}"`);
        finalText = simpleText || '‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡∏£‡∏∞‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πà‡∏á';
      }

    } catch (error) {
      console.error('‚ùå Error during Gemini call:', error);
      finalText = '‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö AI ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πà‡∏á';
    }

    // Create a simulated streaming response from the complete text
    const encoder = new TextEncoder();
    const stream = new ReadableStream({
      async start(controller) {
        try {
          // Get the response text that was generated above
          const responseText = finalText || '‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡∏£‡∏∞‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ';

          // Simulate streaming by sending the complete response in chunks
          const chunkSize = 20; // Send 20 characters at a time
          for (let i = 0; i < responseText.length; i += chunkSize) {
            const chunk = responseText.slice(i, i + chunkSize);
            controller.enqueue(encoder.encode(`0:"${chunk.replace(/"/g, '\\"')}"\n`));
            // Small delay to simulate streaming
            await new Promise(resolve => setTimeout(resolve, 50));
          }
          controller.close();
        } catch (error) {
          console.error('‚ùå Simulated streaming error:', error);
          controller.error(error);
        }
      }
    });

    const totalTime = Date.now() - startTime;
    console.log(`üéØ Total request processing time: ${totalTime}ms`);
    console.log('=== AI CHAT REQUEST END ===');

    return new Response(stream, {
      headers: {
        'Content-Type': 'text/plain; charset=utf-8',
        'Cache-Control': 'no-cache',
        'Connection': 'keep-alive',
      },
    });
  } catch (error) {
    const totalTime = Date.now() - startTime;
    console.error(`‚ùå Chat API error after ${totalTime}ms:`, error);
    console.error('Error stack:', error instanceof Error ? error.stack : 'No stack available');
    console.log('=== AI CHAT REQUEST FAILED ===');
    return new Response(`Internal server error: ${error instanceof Error ? error.message : 'Unknown error'}`, { status: 500 });
  }
}